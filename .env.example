# Environment Configuration
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3
CHUNK_SIZE=2000
CHUNK_OVERLAP=200
TEMPERATURE=0.3
GRADIO_PORT=7860
MAX_CONCURRENT_REQUESTS=3
REQUEST_TIMEOUT=300
