# Environment Configuration
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3
CHUNK_SIZE=2000
CHUNK_OVERLAP=200
TEMPERATURE=0.3
GRADIO_PORT=7860
MAX_CONCURRENT_REQUESTS=3
REQUEST_TIMEOUT=300

# Logging Configuration
# LOG_LEVEL can be: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
